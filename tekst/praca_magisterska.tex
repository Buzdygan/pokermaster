\documentclass[licencjacka]{pracamgr}

\usepackage{polski}
\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
%\usepackage{float}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{setspace}

\singlespacing

\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}

\newenvironment{proof}[1][Dowód]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Przykład]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Uwaga]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
        \ifdim\lastskip<1.5em \hskip-\lastskip
            \hskip1.5em plus0em minus0.5em \fi \nobreak
                  \vrule height0.75em width0.5em depth0.25em\fi}

\allowdisplaybreaks


%\floatstyle{boxed}
%\restylefloat{figure}
%\floatplacement{figure}{H}

\author{Jakub Tlałka}
\nralbumu{292665}

\title{Heurystyczna modyfikacja techniki CFR w Pokerze}
\tytulang{Heuristic modification of CFR technique in Poker}

\kierunek{Informatyka}

\opiekun{dra Jakuba Pawlewicza\\
  Wydzia{\l} Matematyki Informatyki i Mechaniki\\
  }
  
\date{Sierpień 2014}

%TODO poprawić numerek
\dziedzina{
11.3 Informatyka\\
}

%TODO zmienić klasyfikacjż
\klasyfikacja{D. Maths\\
D.0. General\\}

%TODO uzupełnić sżowa kluczowe
\keywords{}
 
\newtheorem{defi}{Definicja}[section]

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\tableofcontents

\chapter{Wstęp}

\noindent
Ludzie grają w gry już od kilku tysięcy lat. Od zawsze były formą rozrywki oraz integracji.
Zarówno gry sportowe jak i umysłowe stwarzają okazje do rywalizacji, która ma charakter pozytywny ponieważ
stymuluje do rozwoju nie tylko jednostki ale i całe społeczności. Rozwój gier komputerowych był równoległy
z rozwojem sprzętu komputerowego. Gry znalazły zastosowanie również w naukach
ekonomicznych. Rozwój teorii gier pomógł zrozumieć procesy rządzące rywalizacją w szeroko rozumianym
znaczeniu. Zdefiniowanie pojęcia równowagi przez amerykańskiego matematyka Johna Nasha było przełomowym
osiągnięciem w dziedzinie badań nad strategiami, nie tylko w grach ale i w ekonomii. \\

\noindent
Wraz z rozwojem technologii komputerowych uzyskano dostęp do narzędzi pozwalających na zastosowanie
teorii do obliczania optymalnych strategii w grach. Powszechnie znana jest rywalizacja
arcymistrzów szachowych z komputerowymi programami. W 1997 roku maszyna "Deep Blue" opracowana przez IBM
specjalnie do celów rywalizacji szachowej, pokonała arcymistrza Garry'ego Kasparova. W przypadku gier
o niewielkim zbiorze stanów, komputery są zdolne obliczyć strategię optymalną, czyli taką która
gwarantuje najlepszy możliwy dla gracza wynik. Przykładem takiej gry są warcaby, w 2007
roku magazyn $Science$ opublikował artykuł dowodzący że programu warcabowego "Chinook" nie da się pokonać. \\

\noindent
Wciąż pozostało jednak wiele gier o rozmiarze tak dużym, że znalezienie dla nich optymalnej strategii wydaje
się poza zasięgiem współczesnych maszyn. Są natomiast bardzo udane próby znajdowania strategii aproksymacyjnych,
liczonych dla abstrakcji rzeczywistych gier, które spisują się coraz lepiej w pojedynkach z ludźmi.
Abstrakcje zawężają zbiór akcji dostępnych graczom i utożsamiają niektóre stany gry w celu zmniejszenia jej rozmiaru.
W trakcie badań nad strategiami rozwinęły się algorytmy takie jak "Monte Carlo Tree Search" czy opisywana
w tej pracy metoda "Counterfactual Regret Minimization" (CFR). Odkrycie tej ostatniej w 2007 roku\cite{cfr} było przełomem
w znajdowaniu aproksymacji równowagi Nasha w grach w postaci ekstensywnej. Metoda opiera się na poprawianiu strategii w kolejnych
iteracjach. Prawdopodobieństwo zagrania akcji w kolejnej iteracji jest proporcjonalne do oczekiwanego zysku z tej akcji
przy założeniu że gracze grają strategią z obecnej iteracji. Modyfikacje metody CFR są używane
do znajdowania najlepszych istniejących strategii m.in w grze Texas Holdem Poker. \\

\noindent
Celem tej pracy jest zbadanie modyfikacji algorytmu CFR, która pozwala na wyliczanie strategii w bardziej wiernej
abstrakcji gry Texas Holdem Poker. Odbywa się to kosztem założenia o pełnej pamięci gry (tzw "perfect recall").
Zignorowanie historii dotychczasowej rozgrywki przy obliczaniu strategii pozwala na znaczące zmniejszenie liczby
stanów gry przeglądanych w każdej iteracji algorytmu. Pozwala to zmniejszyć liczbę ograniczeń nakładanych
na oryginalną wersję Texas Holdem Poker. Praca bada czy tak zmodyfikowany algorytm pozwala na znalezienie
lepszych strategii niż oryginalna wersja CFR. \\

\noindent
Rozdział 2 opisuje podstawowe pojęcia i zagadnienia sztucznej inteligencji w grach takie jak
stan gry, zbiór informacyjny, drzewo gry, strategia. W dalszej części opisane są zasady gry
Texas Holdem Poker, którą wykorzystano do porównania skuteczności metody CFR i jej modyfikacji.
Zostanie też przybliżona historia rozwoju programów grających w gry oraz metody budowania abstrakcji gier. \\

\noindent
Rozdział 3 przedstawia algorytm CFR w wersji pierwszy raz przedstawionej w 2007 roku w pracy
"Regret Minimization in Games with Incomplete Information" \cite{cfr}. Zostały w nim przytoczone pojęcia
związane z grami ekstensywnymi oraz twierdzenia dowodzące skuteczności metody CFR. Na końcu rozdziału
zamieszczony jest pseudokod algorytmu $VanillaCfr$, który jest jedną ze znanych realizacji tej metody. \\

\noindent
Rozdział 4 jest poświęcony modyfikacji metody CFR. Na początku przedstawiona jest idea algorytmu.
Dalej opisana jest wersja gry Texas Holdem Poker zastosowana w eksperymentach. Godny uwagi
jest opisany w tej części pomysł na bardziej efektywne obliczanie metryki $EHS$ dla układów kart.
Wartość $EHS$ wyznacza siłę układu kart. Dzięki jej obliczeniu możliwe jest utożsamianie układów
o podobnej sile. Jest to stosowane w celu zmniejszenia rozmiaru gry, dla której liczona jest strategia.
Zbiór utożsamionych ze sobą układów o podobnej sile nazywamy koszykiem.
Główna część rozdziału jest poświęcona efektywnej realizacji pomysłu na zmniejszenie złożoności
metody CFR. Opisane są modyfikacje algorytmu $VanillaCfr$, które pozwalają zmniejszyć złożoność
jego iteracji do liniowej względem liczby różnych stanów zamiast liniowej względem rozmiaru drzewa gry.
Na końcu zamieszczony został pseudokod zmodyfikowanej wersji algorytmu. \\

\noindent
Rozdział 5 przedstawia wyniki eksperymentalne. Porównane są w nim strategie wyliczone metodą
CFR i jej modyfikacją, przy zastosowaniu kilku wariantów gry. Warianty różnią się pod względem
liczby koszyków, do których przydzielane są układy kart. Podstawowym kryterium
do porównywania jest wynik rzeczywistej rozgrywki Pokera między dwoma programami. \\

\noindent
Rozdział 6 przedstawia podsumowanie wyników pracy. Opisane są zalety badanego podejścia
i możliwości jego poprawy w celu uzyskania jeszcze lepszych strategii.

\chapter{Wprowadzenie do Pokera}

\noindent
W tym rozdziale przybliżona została dziedzina sztucznej inteligencji w grach. Przedstawione
są podstawowe pojęcia i metody. Opisana została gra Texas Holdem Poker, która jest
wykorzystywana do badania algorytmów w tej pracy. Przybliżona została też idea
zmniejszania rozmiaru gry na potrzeby efektywnego obliczania strategii.

\section{Pomocnicze pojęcia}

\begin{itemize}
\item Stan gry jest opisany przez zespół informacji, które są rezultatem dotychczasowych akcji
      podjętych przez graczy oraz zdarzeń losowych. Stan gry jednoznacznie definiuje zbiór dostępnych
      akcji oraz graczy, którzy je wykonują (wliczając w to tzw gracza losowego, który odpowiada
      za generowanie zdarzeń losowych zgodnie z zasadami gry). Wykonywane przez graczy akcje
      zmieniają stan gry (inaczej ich wykonywanie nie ma sensu).
\item Graf gry to graf skierowany, którego wierzchołkami są stany gry a krawędź prowadzi od stanu
      $s_1$ do stanu $s_2$ jeśli w stanie $s_1$ można wykonać akcję prowadzącą do stanu
      $s_2$. Jeśli w takim grafie nie występuje cykl, nazywamy go DAGiem.
\item Drzewo gry reprezentuje wszystkie możliwe rozgrywki. Każdemu wierzchołek reprezentuję jedną historię
      czyli ciąg akcji wykonanych w grze. Wierzchołkowi przypisany jest stan gry, który jest efektem
      wykonania tego ciągu akcji. Z każdego wierzchołka wychodzi tyle krawędzi ile różnych
      akcji można wykonać w stanie gry przypisanym temu wierzchołkowi.
\item Gry dzielimy na dwie kategorie: z pełną informacją oraz z niepełną informacją.
      W przypadku tych pierwszych, wszyscy gracze posiadają wszystkie informacje o zdarzeniach
      zachodzących w grze. Przykładem takiej gry są szachy. W grach z niepełną informacją, niektóre
      zdarzenia są ukryte dla pewnego zbioru graczy. Na przykład w więkości gier karcianych gracze
      nie znają kart swoich przeciwników.
\item Zbiór informacyjny dla gracza $p$ zawiera te stany gry, które są identyczne jeśli chodzi
      o informacje dostępne graczowi $p$. Przykładowo, w dwuosobowej grze karcianej, przykładem zbioru informacyjnego
      jest zbiór wszystkich stanów zaraz po rozdaniu kart, w których gracz $p$ ma na ręku jedynie asa i króla kier.
      Zbiory informacyjne są ważne z punktu widzenia podejmowania decyzji: w każdym stanie gry należącym do tego
      samego zbioru informacyjnego gracza $p$, gracz $p$ podejmuje decyzje wg tej samej strategii.
\item Strategia gracza $p$ wyznacza zasady podejmowania przez gracza akcji w zbiorach informacyjnych.
      Strategia może być deterministyczna, jeśli w każdym zbiorze informacyjnym jest wyznaczona dokładnie
      jedna podejmowana akcja, bądź mieszana jeśli na akcjach określony jest rozkład prawdopodobieństwa
      zagrania ich.
\item Programy, które podejmują akcje w grze zgodnie z zaprogramowaną bądź wyliczoną strategią nazywamy botami
\end{itemize}

\section{Sztuczna inteligencja w grach}

Sztuczna inteligencja w grach to dział algorytmiki zajmujący się badaniem programów grających
z dużą skutecznością w gry. Jest dużo rodzajów gier. Gry jednoosobowe to tzw łamigłówki, w których
należy znaleźć odpowiednią sekwencję ruchów, prowadzącą do jak najlepszego wyniku. Algorytmy znajdujące
takie sekwencje opierają się na efektywnym przeglądaniu drzewa gry. Programy grające w gry wieloosobowe
muszą wyliczyć odpowiednią strategię. Gry różnią się złożonością informacji składających się na stan gry.
Nawet prosta gra, w której graczom przydzielana jest losowo wybrana liczba rzeczywista, mają nieskończenie
wiele stanów gry. Zazwyczaj jednak informacje można reprezentować przez ograniczone liczby całkowite przez co
liczba stanów gry też jest ograniczona. Przedmiotem badań tej pracy jest gra Texas Holdem Poker, która
należy do gatunku gier z niepełną informacją z ograniczoną liczbą stanów gry.

\section{Texas Holdem Poker}

Texas Holdem Poker to gra karciana przeznaczona dla co najmniej dwóch graczy. Na początku pojedynczej rozgrywki każdy z graczy
dostaje dokładnie dwie karty, których nie pokazuje przeciwnikom. Następuje potem runda licytacji, w której
gracze ustalają wysokość stawki. W pierwszej rundzie licytacji gracz zaczynający musi wejść do gry
za stawkę ustaloną wielkością "small blind" a następny gracz w kolejności musi wejść stawką o wysokości
"big blind". \\

\noindent
W licytacji gracze odzywają się w ustalonej kolejności. Mogą wejść za całą dostępną im kwotę (\textbf{all-in}),
podbić stawkę (\textbf{raise}), pozostać przy
obecnej stawce (\textbf{call}) lub zrezygnować z udziału w licytacji (\textbf{fold}). W pierwszych dwóch przypadkach są
zobowiązani oddać do puli stawkę na jaką się zgodzili. Różne wersje gry określają ograniczenia podnoszenia
stawki. W tej pracy przyjęta jest wersja, w której można podnieść stawkę o dowolną kwotę całkowitą o ile stawka
po podniesieniu nie przekracza 128 jednostek. Popularną ale jednocześnie trudną obliczeniowo wersją
jest wersja no-limit, w której nie nakłada się limitu na kwotę przebicia.
Stawka na zakończenie licytacji zostaje ustalona, gdy
wszyscy gracze biorący jeszcze udział w licytacji zaakceptują aktualną wysokość stawki. Jeśli będzie to
tylko jedna osoba, zostaje ona zwycięzcą i zgarnia całą pulę a gra się kończy. W limit Texas Holdem Poker
nakłada się ograniczenie na liczbę zagrywek jednego gracza w licytacji.\\

\noindent
Po pierwszej rundzie licytacji następuje wyłożenie trzech kart na stół, tak by widzieli je wszyscy gracze.
Jest to tzw "flop". Następuje kolejna runda licytacji po czym wykładana jest jedna karta, tzw "turn".
Po trzeciej rundzie licytacji na stół wykładana jest ostatnia karta "river" i następuje ostatnia runda
licytacji. Jeżeli w ostatniej rundzie co najmniej dwóch graczy uzgodni ostateczną stawkę gry, następuje
wyłożenie kart graczy na stół i rozstrzygnięcie kto został zwycięzcą. \\

\noindent
Zwycięzcą zostaje gracz, który jest w stanie wybrać ze swoich kart oraz kart na stole, najsilniejszy
5-kartowy układ. Siłę układu 5 kart wyznacza poniższy ranking, dla każdego typu układu, im wyższe
figury występują w układzie tym większa jest jego siła. Typy układów wymienione są w kolejności
od najsilniejszego do najsłabszego.

\begin{enumerate}
\item \textbf{Poker}: 5 kolejnych kart w tym samym kolorze, np $(D\clubsuit, W\clubsuit, 10\clubsuit, 9\clubsuit, 8\clubsuit)$ 
\item \textbf{Kareta}: 4 takie same figury, np $(7\heartsuit, 7\spadesuit, 7\diamondsuit, 7\clubsuit, D\diamondsuit)$
\item \textbf{Full}: Trójka takich samych figur połączona z parą takich samych figur, np $(K\heartsuit, K\diamondsuit, K\clubsuit, 3\heartsuit, 3\diamondsuit)$ 
\item \textbf{Kolor}: 5 kart w tym samym kolorze, np $(K\diamondsuit, W\diamondsuit, 6\diamondsuit, 5\diamondsuit, 2\diamondsuit)$
\item \textbf{Strit}: 5 kolejnych kart, np $(10\heartsuit, 9\diamondsuit, 8\heartsuit, 7\clubsuit, 6\spadesuit)$
\item \textbf{Trójka}: 3 takie same figury, np $(5\heartsuit, 5\diamondsuit, 5\clubsuit, 8\heartsuit, D\spadesuit)$
\item \textbf{Dwie pary}: Dwie pary tych samych figur, np $(A\heartsuit, A\diamondsuit, 8\clubsuit, 8\heartsuit, W\clubsuit)$
\item \textbf{Para}: Para tych samych figur, np $(10\clubsuit, 10\spadesuit, A\heartsuit, 9\clubsuit, 2\heartsuit)$
\item \textbf{Najwyższa karta}: Najwyższa figura, np $(K\heartsuit, W\diamondsuit, 10\heartsuit, 9\heartsuit, 5\spadesuit)$
\end{enumerate}

\noindent
W sytuacji gdy gracze mają ten sam typ układu, z tymi samymi figurami, pod uwagę brane są pozostałe karty z układu. Np
układ $(7\heartsuit, 7\diamondsuit, 7\clubsuit, K\heartsuit, W\clubsuit)$ jest silniejszy od układu
$(7\spadesuit, 7\diamondsuit, 7\clubsuit, K\spadesuit, 5\diamondsuit)$. \\

\noindent
Jeśli po uwzględnieniu wszystkich 5 kart układu nadal występuje remis (np gdy najsilniejszy układ stanowią karty na stole),
pula jest dzielona między graczy z najwyższym układem.

\section{Rozwój botów grających w Pokera}

Pierwsze boty opierały swoją strategię na zbiorze zasad wpisanych ręcznie przez pokerowych graczy. Zasady określały
sposób podejmowania decyzji w konkretnych sytuacjach, np "mając na ręku Asa i Damę" podbijaj stawkę trzykrotnie".
W 2004 roku powstał program WinHoldem, który umożliwiał proste tworzyenie botów grających według pewnego zbioru zasad.
Jednocześnie, w środowiskach akademickich rozwijały się podejścia oparte na znajdowaniu równowagi Nasha. Owocem tych
badań jest między innymi opisywany w tej pracy algorytm CFR. Takie podejścia przeważają w długiej serii rozgrywek ale
brakuje im umiejętności wykorzystania słabości w strategiach przeciwnika. Powstały również algorytmy, które w serii
rozgrywek próbują estymować strategię przeciwnika i znajdują najlepszą dla niej kontrstrategię. \cite{exploit}

\section{Zmniejszanie drzewa gry}

Drzewo gry w pokerze jest bardzo duże. W dwuosobowym no-limit Texas Holdem Poker jest wielkości
ok. $10^{70}$ stanów. \cite{monte-carlo}. Dlatego niezbędne jest wprowadzenie abstrakcji gry.
Pierwsze ograniczenie, które zastosowano dotyczy licytacji. Dostępne są maksymalnie 4 akcje:
fold (rezygnacja z licytacji), call (pozostanie przy obecnej stawce), raise(dwukrotne podbicie stawki) oraz
all in(zalicytowanie maksymalnej stawki - 128). W ten sposób jedyne możliwe wysokości stawek w licytacji to: $1$, $2$,
$4$, $8$, $16$, $32$, $64$ i $128$. \\\\
\noindent
Drugie ograniczenie przyporządkowuje układom kart graczy (karty na ręce wraz z kartami na stole) tzw "koszyki".
W jednym "koszyku" są karty o podobnej sile. W każdej rundzie licytacji obowiązuje osobny podział, zmienna
jest też liczba koszyków. Testowano różne liczby koszyków, w zależności od używanego algorytmu. \\\\
\noindent
Do oceny siły układów zastosowano miarę "Effective Hand Strength" (EHS). EHS układu wyliczany
jest z trzech innych miar: "Hand Strength" ($HS$), "Positive Potential" ($Ppot$) oraz "Negative Potential" ($Npot$) według
wzoru:

\begin{align*}
EHS = HS \cdot (1 - Npot) + (1 - HS) \cdot Ppot
\end{align*}

\noindent
$HS$ to w przybliżeniu prawdopodobieństwo że nasza ręka jest lepsza od ręki przeciwnika. Miary $Ppot$ i $Npot$ oznaczają
prawdopodobieństwo że nasza ręka stanie się lepsza (odpowiednio gorsza) od ręki przeciwnika po wyłożeniu kolejnych kart na stół. \\



\chapter{Metoda CFR}

Metoda CFR (Counterfactual Regret Minimization) to algorytm, który pozwala znajdować równowagę Nasha
w grach w postaci ekstensywnej. Jest to obecnie najlepsze rozwiązanie w dziedzinie badań nad
sztuczną inteligencją w Pokerze. Choć używane są głównie różne modyfikacje algorytmu, chciałbym
przedstawić tu jego bazową wersję.

\section{Terminologia}

\begin{itemize}

\item Zbiór graczy oznaczamy przez $N$.
\item Zbiór wszystkich możliwych ciągów akcji (historii) legalnych w grze nazywamy $H$. Ciągi, po których kończy
      się rozgrywka, należą do podzbioru $Z$. Oczywiście każdej historii jest przyporządkowany stan gry, który
      jest jej efektem.
\item Dla każdej historii $h$ zbiór akcji dostępnych w stanie gry jej odpowiadającym oznaczamy przez $A(h)$. Oczywiście
      każdy stan wyznacza jednoznacznie gracza wykonującego akcje.
\item Dla każdego gracza $i$, zbiór historii, w których ma on ruch, są pogrupowane w tzw zbiory informacyjne $I_i$. Dla zbioru
      informacyjnego $I_i$ oraz dowolnych dwóch historii $h_1, h_2 \in I_i$ zachodzi $A(h_1) = A(h_2) = A(I_i)$. Do jednego
      zbioru informacyjnego $I_i$ należą historie, które są parami tożsame po usunięciu z nich akcji niewidocznych dla gracza $i$
      (np w Pokerze są to akcje przydzielenia kart prywatnych graczom innym niż $i$).
\item Gracza losowego oznaczamy przez $c$. Funkcja $f_c(h, a)$ oznacza prawdopodobieństwo zaistnienia akcji $a$ dla historii $h$.
\item Strategię gracza $i$ oznaczamy przez $\sigma_i$. $\sigma_i(I_i, a)$ oznacza prawdopodobieństwo wykonania akcji $a$ przez
      gracza $i$ w zbiorze informacyjnym $I_i$ jeśli gra on strategią $\sigma_i$. Przyjmujemy $\sigma_c(h, a) = f_c(h, a)$.
      Przez $\sigma$ oznaczamy zespół strategii wszystkich graczy biorących udział w rozgrywce. Przez $\sigma_{-i}$ oznaczamy
      zespół strategii wszystkich graczy z wyjątkiem $i$.
\item Przez $\pi^{\sigma}(h)$ oznaczamy prawdopodobieństwo zajścia historii $h$ dla zespołu strategii $\sigma$.
      $\pi^{\sigma}(I)$ to suma tych prawdopodobieństw po $h \in I$. $\pi_i^{\sigma}(h)$ to prawdopodobieństwo że grając
      zgodnie ze strategią $\sigma$, gracz $i$ będzie podejmował akcje z odpowiadającej historii $h$. Analogicznie
      $\pi_{-i}^{\sigma}(h)$ oznacza prawdopodobieństwo że pozostali gracze będą podejmowali akcje z historii $h$.
\item Dla końcowych historii $h \in Z$ określamy funkcję użyteczności $\mu_i(h)$ oznaczającą zysk gracza $i$ w rozgrywce
      określonej przez $h$. Dla historii $h \notin H$ określamy $\mu_i^{\sigma}(h)$ oznaczającą oczekiwany zysk gracza $i$
      przy założeniu że gracze grają strategią $\sigma$ i historia $h$ została osiągnięta. Funkcja $\mu^{\sigma}$ wyznacza
      oczekiwane zyski graczy w grze przy ustalonej strategii $\sigma$.

\end{itemize}

\noindent
W dalszej części będziemy zajmować się grami dwuosobowymi, przede wszystkim dwuosobową wersją Texas Holdem Poker.

\section{$\epsilon$-równowaga Nasha}

$\epsilon$-równowagą Nasha nazywamy zespół strategii $\sigma$, taki że:

\begin{align*}
\mu_1^{\sigma} + \epsilon \geq  \max_{\sigma_1'} \, \mu_1^{\sigma_1', \sigma_2} 
\end{align*}

\begin{align*}
\mu_2^{\sigma} + \epsilon \geq  \max_{\sigma_2'} \, \mu_2^{\sigma_1, \sigma_2'} 
\end{align*}

\noindent
Metoda CFR znajduje $\epsilon$-równowagę Nasha dla $\epsilon$ malejącego proporcjonalnie do pierwiastka z liczby iteracji.

\section{Minimalizacja funkcji regretu}

W dalszej części przyjmujemy że rozgrywane są kolejne rundy indeksowane przez $t$. W każdej z rund
strategie graczy będą modyfikowane na podstawie dotychczasowej rozgrywki, w efekcie dając ciąg strategii
$(\sigma^t)$. Jako uśrednioną strategię gracza $i$ po $T$ rundach definiujemy:

\begin{align*}
\overline{\sigma}_i^T(I, a) = \frac{\sum\limits_{t=1}^T \pi_i^{\sigma^t}(I) \, \sigma^t(I, a)}{\sum\limits_{t=1}^T \, \pi_i^{\sigma^t}(I)}
\end{align*}

\noindent
Intuicyjnie funkcja regretu określa ile gracz mógłby zyskać zamieniając obecną strategię na najlepszą możliwą
przy założeniu że inni gracze pozostali by przy obecnych strategiach. Formalnie definiujemy średni całkowity regret gracza $i$
w rozgrywce o numerze $T$ przez:

\begin{align*}
R_i^T = \frac{1}{T} \, \max_{\sigma_i'} \sum\limits_{t=1}^T \, (\mu_i^{\sigma_i', \sigma_{-i}^t} - \mu_i^{\sigma^t})
\end{align*}

%todo dodaj referencje
\noindent
Znany wynik (referencja) mówi o tym, że w grze o sumie zerowej, jeśli po $T$ iteracjach średni całkowity regret obu graczy
jest mniejszy niż $\epsilon$ to uśredniony zespół strategii $\overline{\sigma}^T$ jest $2\epsilon$-równowagą Nasha.

\section{Funkcja regretu lokalnego}

By łatwiej minimalizować regret całkowity, wprowadzamy funkcję regretu lokalnego.
Działa ona na podobnej zasadzie co regret całkowity, ale jest określana na zbiorach informacyjnych
a nie na całej grze. Dzięki temu można ją minimalizować osobno dla każdego zbioru informacyjnego. \\\\

\noindent
Niech $\sigma_{|I \rightarrow a}$ oznacza strategię $\sigma$ zmodyfikowaną tak, że w zbiorze informacyjnym $I$ zawsze
wykonywana jest akcja $a$. $\mu_i(\sigma, I)$ oznacza warunkową wartość oczekiwaną zysku gracza $i$ przy założeniu że
osiągnięty został zbiór informacyjny $I$ a gracze grają strategią $\sigma$ zmodyfikowaną tak, że gracz $i$
podejmuje akcje prowadzące do zbioru informacyjnego $I$ z prawdopodobieństwem $1$. \\\\

\noindent
Wtedy definiujemy:

\begin{align*}
R_i^T(I, a) = \frac{1}{T} \sum\limits_{t=1}^{T} \, \pi_{-i}^{\sigma^t}(I)(\mu_i(\sigma^t_{I \rightarrow a}, I) - \mu_i(\sigma^t, I))
\end{align*}
\begin{align*}
R_{i, imm}^T(I) = \max_{a \in A(I)} \, R_i^T(I, a)
\end{align*}

\noindent
Dzięki twierdzeniu z (referencja) wiemy, że $R_i^T \leq \, \sum_{I} \, max(R_{i, imm}^T(I), 0)$. W takim razie
minimalizowanie regretów w zbiorach informacyjnych minimalizuje regret całkowity i prowadzi do znalezienia
dobrej aproksymacji równowagi Nasha. \\

\noindent
Algorytm CFR w kolejnych iteracjach przechodzi drzewo gry i dla każdego zbioru informacyjnego aktualizuje strategię
zgodnie z regułą:

\begin{align*}
\sigma_i^{T+1} (I, a) = \frac{max(R_i^T(I, a), 0)}{\sum\limits_{a' \in A(I)} \, max(R_i^T(I, a'), 0)}
\end{align*}

\noindent
pod warunkiem, że suma w mianowniku jest dodatnia. W przeciwnym wypadku wszystkie akcje w $I$ mają równe prawdopodobieństwo.
Czyli akcja jest wybierana tym częściej im większa strata - regret, wiąże się z nie wybraniem jej.\\\\

\noindent
Całkowity uśredniony regret maleje proporcjonalnie do pierwiastka z liczby iteracji wykonanych przez algorytm CFR. Niestety
liczba stanów gry i zbiorów informacyjnych nawet w dwuosobowej wersji Texas Holdem Poker jest zbyt duża by można było
zastosować algorytm bezpośrednio. W tym celu wprowadza się abstrakcje pełnej wersji gry, przez ograniczenia w licytacji
oraz dzielenie kart na niewielką liczbę grup (koszyków) ze względu na ich siłę. \\

\section{Implementacja Vanilla CFR}

Użyto rekurencyjnej implementacji algorytmu. W danej iteracji algorytmu, mapy $R$, $S$ oraz $\sigma$ przechowują informacje
dla par (zbiór informacyjny, akcja).
\begin{itemize}
\item $R$ : przechowuje sumę regretu po wszystkich dotychczasowych iteracjach algorytmu
\item $\sigma$ : przechowuje prawdopodobieństwo wykonania akcji w zbiorze informacyjnym zgodnie z obecną strategią
\item $S$ : przechowuje sumę strategii z wszystkich dotychczasowych iteracji, ważoną przez $\pi_i^{\sigma^t}$
\end{itemize}

\noindent
Metoda $WalkTree$ przechodzi drzewo gry i przelicza wartości dla $R$ i $S$. Zwraca parę
($\mu_0$, $\mu_1$) oczekiwanych zysków obu graczy w danym stanie gry przy aktualnej strategii. \\\\

%\Call \Comment \If \ElsIf  \Else \EndIf

\begin{algorithmic}
    \Function{WalkTree}{$game$, $[\pi_0^{\sigma}, \pi_1^{\sigma}, \pi_c ]$}
        \If {$game \rightarrow finalState()$}
            \State \Return $game \rightarrow getUtility()$
        \EndIf
        \State $\mu \gets 0$ 
        \If {$game \rightarrow randomPlayer()$}
            \For {$(randomAction, randomProb) \, \in \, game \rightarrow distribution()$}
                \State $modGame \gets game \rightarrow makeAction(randomAction)$
                \State $modProb \gets [\pi_0^{\sigma}, \pi_1^{\sigma}, \pi_c \cdot randomProb ]$
                \State $\mu(randomAction) \gets \Call{WalkTree}{modGame, modProb}$
                \State $\mu \gets finalUtility + \mu(randomAction) \cdot randomProb$
            \EndFor
        \Else
            \State $p \gets game \rightarrow currentPlayer()$
            \State $o \gets game \rightarrow currentOpponent()$
            \State $I \gets game \rightarrow informationSet()$
            \For {$action \, \in \, game \rightarrow playerActions()$}
                \State $actionProb \gets \sigma(I, action)$
                \State $modGame \gets game \rightarrow makeAction(action)$
                \If {$p = 0$}
                    \State $modProb \gets [\pi_0^{\sigma} \cdot actionProb, \pi_1^{\sigma}, \pi_c]$
                \Else
                    \State $modProb \gets [\pi_0^{\sigma}, \pi_1^{\sigma} \cdot actionProb, \pi_c]$
                \EndIf
                \State $\mu(action) \gets \Call{WalkTree}{modGame, modProb}$
                \State $\mu \gets \mu + \mu(action) \cdot randomProb$
                \State $R(I, action) \gets R(I, action) + \mu_{p}(action) \cdot \pi_{o}^{\sigma} \cdot \pi_c $
                \State $S(I, action) \gets S(I, action) + \pi_{p}^{\sigma} \cdot actionProb $
            \EndFor
            \For {$action \, \in \, game \rightarrow playerActions()$}
                \State $R(I, action) \gets R(I, action) - \mu_{p} \cdot \pi_{o}^{\sigma} \cdot \pi_c $
            \EndFor
        \EndIf
    \State \Return $\mu$
    \EndFunction
\end{algorithmic}


$\,$ \\
Metoda $RecomputeStrategy$ przelicza aktualną i końcową strategię. Prawdopodobieństwo wykonania akcji $a$
w zbiorze informacyjnym $I$ jest proporcjonalne do wartości $max(0, map(I, a))$ gdzie $map=R$ dla strategii
wyliczanej po każdej iteracji a $map=S$ dla strategii końcowej.

\begin{algorithmic}
    \Function{RecomputeStrategy}{$rmap$}
        \State $\sigma \gets 0$
        \ForAll{$I \in InformationSets$}
        \Comment{Dla każdego zbioru informacyjnego $I$}
            \State $sum \gets 0$
            \ForAll {$a \in A(I)$}
            \Comment{Dla każdej akcji $a$ w zbiorze informacyjnym $I$}
                \State $sum \gets sum + max(0, rmap(I, a))$
            \EndFor
            \ForAll {$a \in A(I)$}
            \Comment{Dla każdej akcji $a$ w zbiorze informacyjnym $I$}
                \State $\sigma(I, a) \gets \frac{max(0, rmap(I, a))}{sum}$
            \EndFor
        \EndFor
        \State \Return $\sigma$
    \EndFunction
\end{algorithmic}

$\,$ \\
\noindent
Cały algorytm wygląda następująco: \\

\begin{algorithmic}
    \Function{VanillaCfr}{iterationsNumber}
        \State $R \gets 0$
        \State $S \gets 0$
        \State $\sigma \gets \Call{DefaultStrategy}{}$
        \For {$iterationsNumber$} 
            \State \Call{WalkTree}{newGame(), $[1.0, 1.0, 1.0]$}
            \State $\sigma \gets \Call{RecomputeStrategy}{R}$
        \EndFor
        \State \Return \Call{RecomputeStrategy}{S}
    \EndFunction
\end{algorithmic}

$\,$ \\
\noindent
$DefaultStrategy$ to strategia w której każda akcja jest wykonywana z takim samym prawdopodobieństwem. Metoda
$RecomputeStrategy(R)$ aktualizuje aktualną strategię tak że prawdopodobieństwo zagrania akcji $a$ w zbiorze informacyjnym
$I$ jest proporcjonalne do wartości $R(I, a)$ o ile ta wartość jest nieujemna. Jeśli jest ujemna, prawdopodobieństwo
wynosi $0$. Analogicznie wygląda obliczenie końcowej strategii przy czym patrzymy na wartości $S(I, a)$.

\chapter{Modyfikacja CFR}

\section{Idea}

Drzewo gry pokera jest duże. Nawet przy zastosowaniu bardzo okrojonych abstrakcji, liczba historii przeglądanych w każdej
iteracji algorytmu przekracza milion. Idea modyfikacji algorytmu CFR opiera się na porzuceniu założenia o pełnej pamięci rozgrywki.
W oryginalnej wersji, każdy stan i zbiór informacyjny zawiera całą historię wykonanych w partii zagrań. Algorytm testowany
w tej pracy redukuje liczbę stanów gry przez ignorowanie historii akcji wykonywanych
przez graczy. Stan gry jest opisany przez następujące parametry: \\

\begin{itemize}
\item Aktualny gracz (łącznie z graczem losowym)
\item Aktualna runda (runda licytacji bądź rozdawania kart, jeśli aktualny gracz to gracz losowy)
\item Obecne koszyki obu graczy
\item Liczba odzywek w aktualnej licytacji
\item Proponowana stawka
\item Uzgodniona stawka
\end{itemize}

\noindent
W jednym zbiorze informacyjnym są wszystkie stany, które są tożsame w powyższych parametrach, z wyłączeniem
wiedzy o koszyku przeciwnika. \\

\noindent
Kluczowe dla zmniejszenia złożoności algorytmu było przeformułowanie algorytmu CFR tak, by każdy
stan odwiedzany był tylko raz w jednej iteracji. Dzięki temu złożoność pojedynczej iteracji jest
liniowa względem liczby stanów. Na początku algorytmu tworzony jest graf przejść między
stanami co pozwala uniezależnić iteracje algorytmu od mechaniki gry. Przyspiesza to znacząco
czas obliczeń w pojedynczym stanie.

\section{Tworzenie abstrakcji}

Drzewo gry zmniejszono przez wprowadzenie ograniczeń w licytacji oraz podział układów kart na koszyki.
W każdej rundzie licytacji gracze mają prawo do jednokrotnego podbicia stawki. Łącznie dostępne są cztery akcje:

\begin{itemize}
\item \textbf{fold} - zrezygnowanie z udziału w rozgrywce.
\item \textbf{call} - zgodzenie się na obecną stawkę.
\item \textbf{raise} - zaproponowanie stawki dwukrotnie większej niż obecna
\item \textbf{all in} - zaproponowanie maksymalnej stawki
\end{itemize}

\noindent
Niektóre akcje są niedostępne jeżeli nie mają strategicznego sensu, np \textbf{fold} w przypadku gdy
proponowana stawka nie jest wyższa niż stawka, na którą gracz już się zgodził. \\

\noindent
W celu podzielenia układów na koszyki, układy z jednej rundy są sortowane po wartości $EHS$. Jeżeli mają być podzielone na
$n$ koszyków, to są dzielone tak, by w każdym koszyku znalazła się podobna liczba układów (czyli $\frac{1}{n}$ wszystkich układów). \\

\noindent
Na potrzebę wyliczania strategii liczone są prawdopodobieństwa przejść między parami koszyków. Czyli np dla każdej kombinacji
trójek kart pojawiąjących się na stole na flopie, liczone jest prawdopodobieństwo zmiany koszyka o numerze $A$ z pierwszej
rundy na koszyk $B$ w drugiej rundzie dla każdej takiej pary $(A, B)$. Dodatkowo liczone są rozkłady koszyków
w każdej rundzie, zależne od koszyków z poprzedniej rundy. \\

\noindent
Wartość $EHS$ układu jest wyliczana na podstawie wzoru podanego w rozdziale 2:

\begin{align*}
EHS = HS \cdot (1 - Npot) + (1 - HS) \cdot Ppot
\end{align*}

\noindent
W celu wyliczenia wartości $HS$ dla układu, przeglądane są wszystkie możliwe pary kart u przeciwnika i liczony jest procent
przypadków gdy siła układu jest większa niż siła kart przeciwnika poszerzonych o karty ze stołu występujące w układzie.
By wyliczyć potencjał układu z rund $1$, $2$, $3$ (dla rundy $4$ liczenie potencjału nie ma sensu) przeglądane są wszystkie
możliwe kombinacje następnych kart na stole. Jeżeli po dodaniu następnych kart, układ zmienił się z przegrywającego w
wygrywający, rośnie wartość $Ppot$. W odwrotnej sytuacj rośnie $Npot$.

\noindent
Wyliczenie wartości $EHS$ dla wszystkich możliwych układów wymaga przejrzenia wszystkich możliwych kombinacji 9 kart (2 kart gracza, 2 przeciwnika
oraz 5 kart na stole). Jest to bardzo wymagające obliczeniowo. By skrócić obliczenia, EHS jest wyliczane jedynie dla niektórych układów
kart gracza. Jest to oparte na obserwacji że niektóre układy powinny mieć taki sam $EHS$. Np $EHS$ pary As Kier i As Karo powinien być taki
sam jak $EHS$ pary As Kier, As Trefl albo As Pik, As Karo. 

\begin{align*}
EHS(A\heartsuit, A\spadesuit) = EHS(A\heartsuit, A\diamondsuit) = EHS(A\heartsuit, A\clubsuit) = EHS(A\spadesuit, A\diamondsuit) = ...
\end{align*}

\noindent
Dany układ figur (np $(A,A,D,9,4)$) może występować w wielu wariantach kolorystycznych. Nie wszystkie są równoważne ze względu
na wartość $EHS$ gdyż różnią się potencjałem na bycie układem jednokolorowym. Uwzględniając te różnice obliczam $EHS$ jedynie
dla niektórych wariantów kolorystycznych danego układu figur. \\

\noindent
Każdemu układowi kart ($U$) przyporządkowany jest reprezentant ($R$) - układ figur, który ma taką samą wartość $EHS$.
$R$ oraz $U$ są tożsame ze względu na zbiory figur. Dodatkowo, jeśli $x$ to kolor występujący najczęściej w $U$
to zbiór figur o kolorze $x$ w układzie $U$ jest tożsamy ze zbiorem figur o kolorze $\clubsuit$ w układzie $R$.
Kolory pozostałych kart w układzie $R$ są dowolne ale $\clubsuit$ jest najczęściej występującym kolorem. Wartość
$EHS$ jest obliczana tylko dla jednego takiego układu $R$. Można to rozumieć jako klasę abstrakcji układów kart, gdzie
relacja równoważności jest określona przez powyższe warunki. \\

\noindent
Przykład: \\

\noindent
Reprezentantem układu $(K\heartsuit, W\clubsuit, 9\heartsuit, 9\spadesuit, 7\diamondsuit)$ może być układ
$(K\clubsuit, W\heartsuit, 9\clubsuit, 9\diamondsuit, 7\spadesuit)$ \\

\noindent
Powyższa metoda pozwala nawet stukrotnie zmniejszyć liczbę układów, dla których liczona jest wartość $EHS$.

\section{Modyfikacja algorytmu}

\noindent
Algorytm jest podzielony na dwie części. W pierwszej budowany jest graf stanów gry. W drugiej części
wykonywane są kolejne iteracje obliczające wartość regretów we wszystkich zbiorach informacyjnych oraz
modyfikowana jest strategia. \\

\noindent
W grafie stanów zbiór wierzchołków odpowiada zbiorowi stanów gry. Stany końcowe odpowiadają
sytuacjom, w których rozgrywka się zakończyła i znany jest zysk obu graczy. Krawędź ze stanu $s_1$ do stanu $s_2$
istnieje jeżeli będąc w stanie $s_1$ można wykonać akcję, której rezultatem jest przejście do stanu
$s_2$. W krawędzi zapisujemy informację o akcji z nią powiązanej oraz o prawdopodobieństwie zajścia
tej akcji jeżeli w stanie $s_1$ akcję podejmuje gracz losowy. W innych przypadkach prawdopodobieństwo
zajścia akcji jest wyznaczone przez aktualną strategię gracza wykonującego akcję w $s_1$. \\

\noindent
Graf stanów jest budowany przy użyciu algorytmu DFS. W każdym nowo odwiedzanym stanie
wyliczany jest jego identyfikator na podstawie aktualnych wartości parametrów. Dodatkowo dodawane
są krawędzie prowadzące do sąsiednich stanów, a w stanach końcowych zapisywana jest informacja o
zyskach graczy. \\

\noindent
Dla każdego stanu $s$ może istnieć wiele ciągów akcji (historii), po których wykonaniu gra
znajdzie się w stanie $s$. Niech $H(s)$ oznacza zbiór historii $h$, których rezultatem jest stan $s$.
W algorytmie $VanillaCfr$ każda historia była przetwarzana osobno, tzn aktualizowane były wartości
regretów w zbiorze informacyjnym jej odpowiadającym. Modyfikacja algorytmu grupuje obliczenia z
wszystkich historii w $H(s)$ i wykonuje jedną zbiorczą aktualizację w stanie $s$. \\

\noindent
W algorytmie $VanillaCfr$ wartości map $R$ i $S$ dla historii $h$, zbioru informacyjnego $I(h)$ oraz akcji $a$ są modyfikowane
wg następujących wzorów:

\begin{align*}
R(I(h), a) &\leftarrow R(I(h), a) + (\mu_p^{\sigma}(h \circ a) - \mu_p^{\sigma}(h)) \cdot \pi_{o}^{\sigma}(h) \cdot \pi_c(h) \\
S(I(h), a) &\leftarrow S(I(h), a) + \sigma(I(h), a) \cdot \pi_p^{\sigma}(h)
\end{align*}

\noindent
($p$ to gracz wykonujący akcję a $o$ to przeciwnik) \\

\noindent
Zauważmy, że dla $h_1, h_2 \in H(s)$ zachodzi $\mu^{\sigma}(h_1) = \mu^{\sigma}(h_2) = \mu^{\sigma}(s)$ oraz
$\sigma(I(h_1), a) = \sigma(I(h_2), a) = \sigma(I(s), a). $
Jeśli przez $s(a)$ oznaczymy stan gry po wykonaniu akcji $a$ w stanie $s$, zachodzi: 

\begin{align*}
\sum\limits_{h \in H(s)} (\mu_p^{\sigma}(h \circ a) - \mu_p^{\sigma}(h)) \cdot \pi_o^{\sigma}(h) \cdot \pi_c(h) =
(\mu_p^{\sigma}(s(a)) - \mu_p^{\sigma}(s)) \cdot \sum\limits_{h \in H(s)} \pi_o^{\sigma}(h) \cdot \pi_c(h)
\end{align*}

\noindent
oraz

\begin{align*}
\sum\limits_{h \in H(s)} \sigma(I(h), a) \cdot \pi_p^{\sigma}(h) =
\sigma(I(s), a) \cdot \sum\limits_{h \in H(s)} \pi_p^{\sigma}(h)
\end{align*}

\noindent
Oznaczmy

\begin{align*}
\pi_i^{\sigma}(s) &= \sum\limits_{h \in H(s)} \, \pi_i^{\sigma}(h) \\
\pi_{i,c}^{\sigma}(s) &= \sum\limits_{h \in H(s)} \, \pi_i^{\sigma}(h) \cdot \pi_c(h)
\end{align*}

\noindent
Zgodnie z powyższymi, modyfikacje map $R$ i $S$ wykonywane w algorytmie $VanillaCfr$ są
równoważne z następującymi:

\begin{align*}
R(I(s), a) &\leftarrow R(I(s), a) + (\mu_p^{\sigma}(s(a)) - \mu_p^{\sigma}(s)) \cdot \pi_{o,c}^{\sigma}(s) \\
S(I(s), a) &\leftarrow S(I(s), a) + \sigma(I(s), a) \cdot \pi_p^{\sigma}(s)
\end{align*}

\noindent
wykonywanymi w każdym, nie końcowym stanie $s$, w którym decyzji nie podejmuje gracz losowy. \\

\noindent
Każda iteracja zmodyfikowanego algorytmu podzielona jest na trzy części:
\begin{enumerate}
\item Przeliczenie wartości $\pi_i^{\sigma}(s)$ oraz $\pi_{i, c}^{\sigma}(s)$ w każdym stanie $s$.
\item Przeliczenie wartości $\mu^{\sigma}(s)$ i zaktualizowanie map $R$ i $S$.
\item Zaktualizowanie strategii na podstawie mapy $R$.
\end{enumerate}

\noindent
Wprowadźmy następujące oznaczenia:

\begin{itemize}
\item $P(s)$ - gracz wykonujący akcję w stanie $s$.
\item $\sigma(s_1, s_2)$ - prawdopodobieństwo wykonania akcji prowadzącej ze stanu $s_1$ do $s_2$ zgodnie ze strategią $\sigma$.
\item $\pi(s_1, s_2)$ - prawdopodobieństwo wykonania akcji gracza losowego prowadzącej ze stanu $s_1$ do $s_2$.
\item $E_{(s_1, s_2)}$ - krawędź ze stanu $s_1$ do $s_2$.
\end{itemize}

\noindent
Zdefiniujmy funkcję $prob_i(E_{(s_1, s_2)})$ następująco 

\begin{equation*}
prob_i(E_{(s_1, s_2)}) = \begin{cases}
                            1                & P(s_1) \neq i     \\
                            \pi(s_1, s_2)    & P(s_1) = i = c    \\
                            \sigma(s_1, s_2) & P(s_1) = i \neq c \\
                          \end{cases}
\end{equation*}

\noindent
Wtedy zachodzi:

\begin{align*}
\pi_i^{\sigma}(s) &= \sum\limits_{E_{(s', s)}} \pi_i^{\sigma}(s') \cdot prob_i(E_{(s', s)}) \\
\pi_{i, c}^{\sigma}(s) &= \sum\limits_{E_{(s', s)}} \pi_{i, c}^{\sigma}(s') \cdot prob_i(E_{(s', s)}) \cdot prob_c(E_{(s', s)})
\end{align*}

\noindent
Czyli żeby obliczyć wartości $\pi_i^{\sigma}(s)$ oraz $\pi_{i, c}^{\sigma}(s)$ w stanie $s$, wystarczy
mieć je policzone dla każdego stanu $s'$ z którego prowadzi krawędź do $s$. W stanie początkowym $s_0$ zachodzi
$\pi_i^{\sigma}(s_0) = 1$ oraz $\pi_{i, c}^{\sigma}(s_0) = 1$. Graf stanów to tzw "DAG", czyli
skierowany graf acykliczny. W takim grafie wierzchołki można ułożyć w ciąg w porządku topologicznym, tzn
jeżeli z wierzchołka $v$ prowadzi krawędź do wierzchołka $w$ to $v$ znajduje się w ciągu przed wierzchołkiem $w$.

\section{Pseudokod zmodyfikowanego algorytmu}

\noindent
Metoda $ComputeProbs$ przegląda stany w porządku topologicznym i oblicza wartości $\pi_i^{\sigma}(s)$ oraz $\pi_{i, c}^{\sigma}(s)$
dla każdego stanu $s$ i gracza $i \in \left\{0, 1\right\}$. \\ 

\begin{algorithmic}
    \Function{ComputeProbs}{}
        \State $\pi_i^{\sigma}(s) \gets 0$ $ \; \pi_{i,c}^{\sigma}(s) \gets 0$
        \Comment{$\forall_{i \in \left\{0, 1\right\} ,\, s \, \in \, States}$}
        \State $\pi_i^{\sigma}(s_0) \gets 1$ $ \; \pi_{i,c}^{\sigma}(s_0) \gets 1$
        \Comment{$\forall_{i \in \left\{0, 1\right\}}$}
        \ForAll {$s \, \in \, States$}
        \Comment{stany przeglądane w porządku topologicznym}
            \ForAll{$s' \, \in \, Neighbours(s)$}
            \Comment{Przegląda stany sąsiadujące z $s$}
                \State $\pi_i^{\sigma}(s') \gets \pi_i^{\sigma}(s') \, + \, \pi_i^{\sigma}(s) \cdot prob_i(E_{(s, s')})$ 
                \Comment{$\forall_{i \in \left\{0, 1\right\}}$}
                \State $\pi_{i,c}^{\sigma}(s') \gets \pi_{i,c}^{\sigma}(s') \, + \, \pi_{i,c}^{\sigma}(s) \cdot prob_i(E_{(s, s')}) \cdot prob_c(E_{(s, s')})$ 
                \Comment{$\forall_{i \in \left\{0, 1\right\}}$}
            \EndFor
        \EndFor
    \EndFunction
\end{algorithmic}

$\,$ \\

\noindent
Metoda $WalkTree$ przegląda rekurencyjnie graf stanów, obliczając oczekiwane wartości zysku graczy oraz
aktualizując mapy $R$ oraz $S$. Każdy stan jest odwiedzany tylko raz. \\

\begin{algorithmic}
    \Function{WalkTree}{$s$}
    \Comment{$s$ to aktualnie odwiedzany stan}
        \If{final-state($s$)}
        \Comment{Jeśli $s$ to stan końcowy}
            \State \Return $\mu(s)$
        \EndIf
        \If{visited($s$)}
        \Comment{Jeśli $s$ został już odwiedzony w tej iteracji}
            \State \Return $\mu^{\sigma}(s)$
        \EndIf
        \State $\mu \gets 0$ 
        \State $p \gets P(s)$
        \State $I \gets I(s)$
        \ForAll{$s' \, \in \, Neighbours(s)$}
        \Comment{Przegląda stany sąsiadujące z $s$}
            \State $a \gets action(E_{(s, s')})$
            \Comment $action(E_{(s, s')})$ to akcja prowadząca ze stanu $s$ do $s'$
            \State $\mu^{s'} \gets \Call{WalkTree}{s'}$
            \State $\mu \gets \mu + \mu^{s'} \cdot prob_p(E_{(s, s')})$
            \If{$p \neq c$}
                \State $R(I, a) \gets R(I, a) + \mu^{s'}_p \cdot \pi_{p,c}^{\sigma}(s)$
                \State $S(I, a) \gets S(I, a) + \sigma(I, a) \cdot \pi_p^{\sigma}(s)$
            \EndIf
        \EndFor
        \If{$p \neq c$}
            \State $R(I, a) \gets R(I, a) - \mu_p \cdot \pi_{p,c}^{\sigma}(s)$
        \EndIf
        \State $\mu^{\sigma}(s) \gets \mu$
        \State \Return $\mu$
    \EndFunction
\end{algorithmic}

$\,$ \\

\noindent
Cały algorytm wygląda następująco: \\

\begin{algorithmic}
    \Function{ModCfr}{$game$, $iterationsNumber$}
        \State $initialState \gets \Call{BuildTree}{game}$
        \State $R \gets 0$
        \State $S \gets 0$
        \State $\sigma \gets \Call{DefaultStrategy}{}$
        \For {$iterationsNumber$} 
            \State $\Call{ComputeProbs}{}()$
            \State $\Call{WalkTree}{initialState}$
            \State $\sigma \gets \Call{RecomputeStrategy}{R}$
        \EndFor
        \State \Return $\Call{RecomputeStrategy}{S}$
    \EndFunction
\end{algorithmic}


\chapter{Wyniki eksperymentalne}

\chapter{Podsumowanie}

\bibliographystyle{plain} 
\bibliography{cfr.bib} 

\end{document}
