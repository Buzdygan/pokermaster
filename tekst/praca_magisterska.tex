\documentclass[licencjacka]{pracamgr}

\usepackage{polski}
\usepackage{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
%\usepackage{float}
\usepackage{wrapfig}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{setspace}

\singlespacing

\newtheorem{theorem}{Twierdzenie}[chapter]
\newtheorem{lemma}[theorem]{Lemat}
\newtheorem{proposition}[theorem]{Stwierdzenie}
\newtheorem{corollary}[theorem]{Wniosek}
\newtheorem{definition}[theorem]{Definicja}

\newenvironment{proof}[1][Dowód]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Przykład]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Uwaga]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
        \ifdim\lastskip<1.5em \hskip-\lastskip
            \hskip1.5em plus0em minus0.5em \fi \nobreak
                  \vrule height0.75em width0.5em depth0.25em\fi}

\allowdisplaybreaks


%\floatstyle{boxed}
%\restylefloat{figure}
%\floatplacement{figure}{H}

\author{Jakub Tlałka}
\nralbumu{292665}

\title{Heurystyczna modyfikacja techniki CFR w Pokerze}
\tytulang{Heuristic modification of CFR technique in Poker}

\kierunek{Informatyka}

\opiekun{dra Jakuba Pawlewicza\\
  Wydzia{\l} Matematyki Informatyki i Mechaniki\\
  }
  
\date{Sierpież 2014}

%TODO poprawić numerek
\dziedzina{
11.3 Informatyka\\
}

%TODO zmienić klasyfikacjż
\klasyfikacja{D. Maths\\
D.0. General\\}

%TODO uzupełnić sżowa kluczowe
\keywords{}
 
\newtheorem{defi}{Definicja}[section]

\begin{document}
\maketitle

\begin{abstract}
\end{abstract}

\tableofcontents

\chapter{Wstęp}

\section{Sztuczna inteligencja w grach}

Sztuczna inteligencja w grach to dział algorytmiki zajmujący się badaniem programów grających
z dużą skutecznością w gry. Jest dużo rodzajów gier. Gry jednoosobowe to tzw łamigłówki, w których
należy znaleźć odpowiednią sekwencję ruchów, prowadzącą do jak najlepszego wyniku. Algorytmy znajdujące
takie sekwencje opierają się na efektywnym przeglądaniu drzewa gry, czyli 


\chapter{Metoda CFR}

Metoda CFR (Counterfactual Regret Minimization) to algorytm, który pozwala znajdować równowagę Nasha
w grach w postaci ekstensywnej. Jest to obecnie najlepsze rozwiązanie w dziedzinie badań nad
sztuczną inteligencją w Pokerze. Choć używane są głównie różne modyfikacje algorytmu, chciałbym
przedstawić tu jego bazową wersję.

\section{Terminologia}

\begin{itemize}

\item Zbiór graczy oznaczamy przez $N$.
\item Zbiór wszystkich możliwych ciągów akcji (historii) legalnych w grze nazywamy $H$. Ciągi, po których kończy
      się rozgrywka, należą do podzbioru $Z$. Oczywiście każdej historii jest przyporządkowany stan gry, który
      jest jej efektem.
\item Dla każdej historii $h$ zbiór akcji dostępnych w stanie gry jej odpowiadającym oznaczamy przez $A(h)$. Oczywiście
      każdy stan wyznacza jednoznacznie gracza wykonującego akcje.
\item Dla każdego gracza $i$, zbiór historii, w których ma on ruch, są pogrupowane w tzw zbiory informacyjne $I_i$. Dla zbioru
      informacyjnego $I_i$ oraz dowolnych dwóch historii $h_1, h_2 \in I_i$ zachodzi $A(h_1) = A(h_2) = A(I_i)$. Do jednego
      zbioru informacyjnego $I_i$ należą historie, które są parami tożsame po usunięciu z nich akcji niewidocznych dla gracza $i$
      (np w Pokerze są to akcje przydzielenia kart prywatnych graczom innym niż $i$).
\item Gracza losowego oznaczamy przez $c$. Funkcja $f_c(h, a)$ oznacza prawdopodobieństwo zaistnienia akcji $a$ dla historii $h$.
\item Strategię gracza $i$ oznaczamy przez $\sigma_i$. $\sigma_i(I_i, a)$ oznacza prawdopodobieństwo wykonania akcji $a$ przez
      gracza $i$ w zbiorze informacyjnym $I_i$ jeśli gra on strategią $\sigma_i$. Przyjmujemy $\sigma_c(h, a) = f_c(h, a)$.
      Przez $\sigma$ oznaczamy zespół strategii wszystkich graczy biorących udział w rozgrywce. Przez $\sigma_{-i}$ oznaczamy
      zespół strategii wszystkich graczy z wyjątkiem $i$.
\item Przez $\pi^{\sigma}(h)$ oznaczamy prawdopodobieństwo zajścia historii $h$ dla zespołu strategii $\sigma$.
      $\pi^{\sigma}(I)$ to suma tych prawdopodobieństw po $h \in I$. $\pi_i^{\sigma}(h)$ to prawdopodobieństwo że grając
      zgodnie ze strategią $\sigma$, gracz $i$ będzie podejmował akcje z odpowiadającej historii $h$. Analogicznie
      $\pi_{-i}^{\sigma}(h)$ oznacza prawdopodobieństwo że pozostali gracze będą podejmowali akcje z historii $h$.
\item Dla końcowych historii $h \in Z$ określamy funkcję użyteczności $u_i(h)$ oznaczającą zysk gracza $i$ w rozgrywce
      określonej przez $h$.

\end{itemize}

\noindent
W dalszej części będziemy zajmować się grami dwuosobowymi, przede wszystkim dwuosobową wersją Texas Holdem Poker.

\section{$\epsilon$-równowaga Nasha}

$\epsilon$-równowagą Nasha nazywamy zespół strategii $\sigma$, taki że:

\begin{align*}
u_1(\sigma) + \epsilon \geq  \max_{\sigma_1'} \, u_1(\sigma_1', \sigma_2) 
\end{align*}

\begin{align*}
u_2(\sigma) + \epsilon \geq  \max_{\sigma_2'} \, u_2(\sigma_1, \sigma_2') 
\end{align*}

\noindent
Metoda CFR znajduje $\epsilon$-równowagę Nasha dla $\epsilon$ malejącego proporcjonalnie do pierwiastka z liczby iteracji.

\section{Minimalizacja funkcji regretu}

W dalszej części przyjmujemy że rozgrywane są kolejne rundy indeksowane przez $t$. W każdej z rund
strategie graczy będą modyfikowane na podstawie dotychczasowej rozgrywki, w efekcie dając ciąg strategii
$(\sigma^t)$. Jako uśrednioną strategię gracza $i$ po $T$ rundach definiujemy:

\begin{align*}
\overline{\sigma}_i^T(I, a) = \frac{\sum\limits_{t=1}^T \pi_i^{\sigma^t}(I) \, \sigma^t(I, a)}{\sum\limits_{t=1}^T \, \pi_i^{\sigma^t}(I)}
\end{align*}

\noindent
Intuicyjnie funkcja regretu określa ile gracz mógłby zyskać zamieniając obecną strategię na najlepszą możliwą
przy założeniu że inni gracze pozostali by przy obecnych strategiach. Formalnie definiujemy średni całkowity regret gracza $i$
w rozgrywce o numerze $T$ przez:

\begin{align*}
R_i^T = \frac{1}{T} \, \max_{\sigma_i'} \sum\limits_{t=1}^T \, (u_i(\sigma_i', \sigma_{-i}^t) - u_i(\sigma^t))
\end{align*}

%todo dodaj referencje
\noindent
Znany wynik (referencja) mówi o tym, że w grze o sumie zerowej, jeśli po $T$ iteracjach średni całkowity regret obu graczy
jest mniejszy niż $\epsilon$ to uśredniony zespół strategii $\overline{\sigma}^T$ jest $2\epsilon$-równowagą Nasha.

\section{Funkcja regretu lokalnego}

By łatwiej minimalizować regret całkowity, wprowadzamy funkcję regretu lokalnego.
Działa ona na podobnej zasadzie co regret całkowity, ale jest określana na zbiorach informacyjnych
a nie na całej grze. Dzięki temu można ją minimalizować osobno dla każdego zbioru informacyjnego. \\\\

\noindent
Niech $\sigma_{|I \rightarrow a}$ oznacza strategię $\sigma$ zmodyfikowaną tak, że w zbiorze informacyjnym $I$ zawsze
wykonywana jest akcja $a$. $u_i(\sigma, I)$ oznacza warunkową wartość oczekiwaną zysku gracza $i$ przy założeniu że
osiągnięty został zbiór informacyjny $I$ a gracze grają strategią $\sigma$ zmodyfikowaną tak, że gracz $i$
podejmuje akcje prowadzące do zbioru informacyjnego $I$ z prawdopodobieństwem $1$. \\\\

\noindent
Wtedy definiujemy:

\begin{align*}
R_i^T(I, a) = \frac{1}{T} \sum\limits_{t=1}^{T} \, \pi_{-i}^{\sigma^t}(I)(u_i(\sigma^t_{I \rightarrow a}, I) - u_i(\sigma^t, I))
\end{align*}
\begin{align*}
R_{i, imm}^T(I) = \max_{a \in A(I)} \, R_i^T(I, a)
\end{align*}

\noindent
Dzięki twierdzeniu z (referencja) wiemy, że $R_i^T \leq \, \sum_{I} \, max(R_{i, imm}^T(I), 0)$. W takim razie
minimalizowanie regretów w zbiorach informacyjnych minimalizuje regret całkowity i prowadzi do znalezienia
dobrej aproksymacji równowagi Nasha. \\

\noindent
Algorytm CFR w kolejnych iteracjach przechodzi drzewo gry i dla każdego zbioru informacyjnego aktualizuje strategię
zgodnie z regułą:

\begin{align*}
\sigma_i^(T+1) (I, a) = \frac{max(R_i^T(I, a), 0)}{\sum\limits_{a' \in A(I)} \, max(R_i^T(I, a'), 0)}
\end{align*}

\noindent
pod warunkiem, że suma w mianowniku jest dodatnia. W przeciwnym wypadku wszystkie akcje w $I$ mają równe prawdopodobieństwo.
Czyli akcja jest wybierana tym częściej im większa strata - regret, wiąże się z nie wybraniem jej.\\\\

\noindent
Całkowity uśredniony regret maleje proporcjonalnie do pierwiastka z liczby iteracji wykonanych przez algorytm CFR. Niestety
liczba stanów gry i zbiorów informacyjnych nawet w dwuosobowej wersji Texas Holdem Poker jest zbyt duża by można było
zastosować algorytm bezpośrednio. W tym celu wprowadza się abstrakcje pełnej wersji gry, przez ograniczenia w licytacji
oraz dzielenie kart na niewielką liczbę grup (koszyków) ze względu na ich siłę. \\

\chapter{Implementacja}

\section{Informacje ogólne}

Całość została zaprogramowana w języku C++. Do zaimplementowania zasad Texas Holdem Pokera oraz jego
abstrakcji wykorzystano mechanizm dziedziczenia. Dla zasad pokera przyjęto 3-fazową licytację w każdej
rundzie. Pierwszy gracz w rundzie flop musi zawsze zagrać za $1$. Podbijać można tylko w $1$ i $2$ fazie
każdej rundy. Karty rozdawane są zgodnie z zasadami Texas Holdem Poker. Do porównywania siły rąk graczy
wykorzystano gotowy algorytm 2+2.

\section{Abstrakcje}

W celu ograniczenia rozmiaru drzewa gry zastosowano abstrakcje. Pierwsze ograniczenie dotyczy
licytacji. Dostępne są maksymalnie 3 akcje: fold (rezygnacja z licytacji), call (pozostanie przy obecnej stawce)
oraz raise(dwukrotne podbicie stawki). W ten sposób jedyne możliwe wysokości stawek w licytacji to: $1$, $2$,
$4$, $8$, $16$, $32$, $64$ i $128$. \\\\
\noindent
Drugie ograniczenie przyporządkowuje układom kart graczy (karty na ręce wraz z kartami na stole) tzw "koszyki".
W jednym "koszyku" są karty o podobnej sile. W każdej rundzie licytacji obowiązuje osobny podział, zmienna
jest też liczba koszyków. Testowano różne liczby koszyków, w zależności od używanego algorytmu.

\section{Obliczanie EHS}

Zaimplementowano osobny moduł, który odpowiada za przyporządkowywanie "koszyków" układom kart.
Są 4 rodzaje układów: pre-flop (2 karty), flop(5 kart), turn(6 kart) i river(7 kart).
Do oceny siły układów zastosowano miarę "Effective Hand Strength" (EHS). EHS układu wyliczany
jest z trzech innych miar: "Hand Strength" ($HS$), "Positive Potential" ($Ppot$) oraz "Negative Potential" ($Npot$) według
wzoru:

\begin{align*}
EHS = HS \cdot (1 - Npot) + (1 - HS) \cdot Ppot
\end{align*}

\noindent
$HS$ to w przybliżeniu prawdopodobieństwo że nasza ręka jest lepsza od ręki przeciwnika. Miary $Ppot$ i $Npot$ oznaczają
prawdopodobieństwo że nasza ręka stanie się lepsza (odpowiednio gorsza) od ręki przeciwnika po wyłożeniu kolejnych kart na stół. \\

\noindent
Wyliczenie wartości $EHS$ dla wszystkich możliwych układów wymaga przejrzenia wszystkich możliwych kombinacji 9 kart (2 kart gracza, 2 przeciwnika
oraz 5 kart na stole). Jest to bardzo wymagające obliczeniowo. By skrócić obliczenia, EHS jest wyliczane jedynie dla niektórych układów
kart gracza. Jest to oparte na obserwacji że niektóre układy powinny mieć taki sam $EHS$. Np $EHS$ pary As Kier i As Karo powinien być taki
sam jak $EHS$ pary As Kier, As Trefl albo As Pik, As Karo. 

\begin{align*}
(A\heartsuit, A\spadesuit) = (A\heartsuit, A\diamondsuit) = (A\heartsuit, A\clubsuit) = (A\spadesuit, A\diamondsuit) =
(A\spadesuit, A\clubsuit) = (A\diamondsuit, A\clubsuit)
\end{align*}

\noindent
W ogólności, wyliczając $EHS$ układów operuję nie na pełnym zbiorze $52$ kart a na zbiorze $13$ figur. Dany układ figur
może występować w kilku wersjach uwzględniających kolory kart. Para tych samych figur jest zawsze różnokolorowa ale w 
parze różnych figur mogą one występować w tym samym kolorze bądź w różnych kolorach i wpływa to na $EHS$ takich układów.

\begin{align*}
EHS(A\heartsuit, K\heartsuit) \neq EHS(A\heartsuit, K\spadesuit)
\end{align*}

\noindent
Stosując tę koncepcję wprowadzono klasy abstrakcji dla układów kart. Układy kart dochodzące w kolejnych rundach są traktowane
osobno. Poniżej typy układów dla poszczególnych rund, napis $Fnx$ oznacza figurę numer $n$ w kolorze $x$. 

\noindent
\textbf{Pre-flop}:
\begin{itemize}
\item (F1a, F1b)
\item (F1a, F2a)
\item (F1a, F2b)
\end{itemize}

\noindent
\textbf{Flop}:
\begin{itemize}
\item (F1a, F1b, F1c)
\item (F1a, F1b, F2a)
\item (F1a, F1b, F2c)
\item (F1a, F2a, F3a)
\item (F1a, F2a, F3b)
\item (F1a, F2b, F3c)
\end{itemize}

\noindent
Dla rund Turn i River w których dochodzi po jednej karcie, każda klasa abstrakcji to czwórka kart o tej samej figurze. \\

\noindent
W celu wyliczenia wartości $HS$ dla układu, przeglądane są wszystkie możliwe pary kart u przeciwnika i liczony jest procent
przypadków gdy siła układu jest większa niż siła kart przeciwnika poszerzonych o karty ze stołu występujące w układzie.
By wyliczyć potencjał układu z rund $1$, $2$, $3$ (dla rundy $4$ liczenie potencjału nie ma sensu) przeglądane są wszystkie
możliwe kombinacje następnych kart na stole. Jeżeli po dodaniu następnych kart, układ zmienił się z przegrywającego w
wygrywający, rośnie wartość $Ppot$. W odwrotnej sytuacj rośnie $Npot$.

\section{Podział na koszyki}

W celu podzielenia układów na koszyki, układy z jednej rundy są sortowane po wartości $EHS$. Jeżeli mają być podzielone na
$n$ koszyków, to są dzielone tak, by w każdym koszyku znalazła się podobna liczba układów (czyli $\frac{1}{n}$ wszystkich układów). \\
\noindent
Na potrzebę wyliczania strategii liczone są prawdopodobieństwa przejść między parami koszyków. Czyli np dla każdej kombinacji
trójek kart pojawiąjących się na stole na flopie, liczone jest prawdopodobieństwo zmiany koszyka o numerze $A$ z pierwszej
rundy na koszyk $B$ w drugiej rundzie dla każdej takiej pary $(A, B)$. Dodatkowo liczone są rozkłady koszyków
w każdej rundzie, zależne od koszyków z poprzedniej rundy.

\section{Implementacja Vanilla CFR}

Użyto rekurencyjnej implementacji algorytmu. W danej iteracji algorytmu, mapy $R$, $S$ oraz $\sigma$ przechowują informacje
dla par (zbiór informacyjny, akcja).
\begin{itemize}
\item $R$ : przechowuje sumę regretu po wszystkich dotychczasowych iteracjach algorytmu
\item $\sigma$ : przechowuje prawdopodobieństwo wykonania akcji w zbiorze informacyjnym zgodnie z obecną strategią
\item $S$ : przechowuje sumę strategii z wszystkich dotychczasowych iteracji, ważoną przez $\pi_i^{\sigma^t}$
\end{itemize}

\noindent
Metoda $WalkTree$ przechodzi drzewo gry i przelicza wartości dla $R$ i $S$. Zwraca parę
($\mu_0$, $\mu_1$) oczekiwanych zysków obu graczy w danym stanie gry przy aktualnej strategii. \\\\

%\Call \Comment \If \ElsIf  \Else \EndIf

\begin{algorithmic}
    \Function{WalkTree}{$game$, $[\pi_0^{\sigma}, \pi_1^{\sigma}, \pi_c ]$}
        \If {$game \rightarrow finalState()$}
            \State \Return $game \rightarrow getUtility()$
        \EndIf
        \State $finalUtility \gets 0$ 
        \If {$game \rightarrow randomPlayer()$}
            \For {$(randomAction, randomProb) \, \in \, game \rightarrow distribution()$}
                \State $modGame \gets game \rightarrow makeAction(randomAction)$
                \State $modProb \gets [\pi_0^{\sigma}, \pi_1^{\sigma}, \pi_c \cdot randomProb ]$
                \State $actionUtility \gets \Call{WalkTree}{modGame, modProb}$
                \State $finalUtility \gets finalUtility + actionUtility \cdot randomProb$
            \EndFor
        \Else
            \State $ply \gets game \rightarrow currentPlayer()$
            \State $opp \gets game \rightarrow currentOpponent()$
            \State $I \gets game \rightarrow informationSet()$
            \For {$action \, \in \, game \rightarrow playerActions()$}
                \State $actionProb \gets \sigma(I, action)$
                \State $modGame \gets game \rightarrow makeAction(action)$
                \If {$ply = 0$}
                    \State $modProb \gets [\pi_0^{\sigma} \cdot actionProb, \pi_1^{\sigma}, \pi_c]$
                \Else
                    \State $modProb \gets [\pi_0^{\sigma}, \pi_1^{\sigma} \cdot actionProb, \pi_c]$
                \EndIf
                \State $actionUtility \gets \Call{WalkTree}{modGame, modProb}$
                \State $finalUtility \gets finalUtility + actionUtility \cdot randomProb$
                \State $R(I, action) \gets R(I, action) + actionUtility_{ply} \cdot \pi_{opp}^{\sigma} \cdot \pi_c $
                \State $S(I, action) \gets S(I, action) + actionProb $
            \EndFor
            \For {$action \, \in \, game \rightarrow playerActions()$}
                \State $R(I, action) \gets R(I, action) - finalUtility_{ply} \cdot \pi_{opp}^{\sigma} \cdot \pi_c $
            \EndFor
        \EndIf
    \State \Return finalUtility
    \EndFunction
\end{algorithmic}

$\,$ \\
\noindent
Cały algorytm wygląda następująco: \\

\begin{algorithmic}
    \Function{VanillaCfr}{iterationsNumber}
        \State $R \gets 0$
        \State $S \gets 0$
        \State $\sigma \gets \Call{DefaultStrategy}{}$
        \For {$iterationsNumber$} 
            \State \Call{WalkTree}{newGame(), $[1.0, 1.0, 1.0]$}
            \State $\sigma \gets \Call{RecomputeStrategy}{R}$
        \EndFor
        \State \Return \Call{RecomputeStrategy}{S}
    \EndFunction
\end{algorithmic}

$\,$ \\
\noindent
$DefaultStrategy$ to strategia w której każda akcja jest wykonywana z takim samym prawdopodobieństwem. Metoda
$RecomputeStrategy(R)$ aktualizuje aktualną strategię tak że prawdopodobieństwo zagrania akcji $a$ w zbiorze informacyjnym
$I$ jest proporcjonalne do wartości $R(I, a)$ o ile ta wartość jest nieujemna. Jeśli jest ujemna, prawdopodobieństwo
wynosi $0$. Analogicznie wygląda obliczenie końcowej strategii przy czym patrzymy na wartości $S(I, a)$.

\section{Zmodyfikowany CFR}

\chapter{Symulacje na dwóch wariantach Pokera}

\chapter{Podsumowanie}


\end{document}
